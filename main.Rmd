---
title: "Insurance Cross Selling (Kaggle)"
author: "Ellis Hodgdon"
date: "2024-08-08"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning=FALSE)
```

### Introduction, setup, loading necessary libraries, and adjust working directories   
```{r}
NUMBER_OF_TEST_SETS = 10
NUMBER_OF_TRAINING_ROWS = 50000
NUMbER_OF_TEST_ROWS = 5000

```


```{r load libraries, echo=FALSE}
if (!require(tidyverse)) suppressMessages(install.packages(tidyverse, verbose=FALSE, quiet = TRUE))
if (!require(rvest)) install.packages("rvest", verbose=FALSE)
if (!require(dplyr)) instsall.packages("dplyr", verbose=FALSE)
if (!require(tidyr)) install.packages("tidyr", verbose=FALSE)
if (!require(caret)) install.packages("caret", verbose=FALSE)
if (!require(caret)) install.packages("caret", verbose=FALSE)
if (!require(rlist)) suppressMessages(install.packages("rlist", verbose=FALSE))
if (!require(randomForest)) suppressMessages(install.packages("randomForest", verbose=FALSE))

getwd()
```
## Problem Definition
The goal of this analysis is to predict which customers respond positively to an automobile insurance offer.

The column *Result* as integer (0 or 1) indicates whether a resale was successful with this customer.


### Input the training dataset and data prep
Kaggle provides two datasets -- training and test -- but the test dataset is only used to judge the developed algorithm. It is missing the *Response* column.We will do some work on the entire training dataset and then split it into two smaller datasets that we can reasonably handle.
```{r read datasets, echo = FALSE}
train_data <- read.csv("insurance-train.csv")


data_prep <- function(df, nzv_arg = FALSE, response = TRUE) {
  # check for any rows containing NAs
  na_counts <- train_data %>% summarise_all(~ sum(is.na(.)))
  number_of_NAs <- sum(na_counts)
  if (number_of_NAs > 0 ) {
    df <- na.omit(df)
    number_of_NAs
  }
   # A value of zero indicates there are no NAs in the entire dataset. Next, check of missing data

   # Check for any rows containing blanks
  na_nulls <- sum(is.null(train_data))
  number_of_nulls <- sum(na_nulls)
  if (number_of_nulls > 0) {
    df[df == "NULL"] <- NA
    df <- na.omit(df)
    number_of_nulls
  }

  # Using the nearZeroVar() function from the caret package,we can determine which predictors are near zero variance and therefore would not be a
  # good predictor.
  ifelse (!nzv_arg,   nzv <- nearZeroVar(train_data), nzv <- nzv_arg)
  if (length(nzv) > 0) {
    for (i in nzv) print(colnames(train_data)[i])
    df <- df[, -nzv]  # remove near zero variances
  }    # end of if (length(nzv) ...
  # This shows that Driving_License and annual premium are not going to be a good predictor so we should remove them from both the training and the   # testing datasets.
  
  # convert the Gender column from "Male", "Female" to 0 and 1df
  df$Gender <- ifelse(df$Gender == "Male", 0, 1)
  
  # convert the Vehicle_Damage column from "Yes" and "No" ti 1 and 0
  df$Vehicle_Damage <- ifelse(df$Vehicle_Damage == "Yes", 1, 0)
  
  # convert the Vehicle_Age from a text to a numeric value
  unique <- unique(train_data$Vehicle_Age)
  for (age in unique) {df$Vehicle_Age[df$Vehicle_Age == age] <- which(unique == age)}
  
  #At this point all values in the data frame are numeric


 return (df)
}   # end of function

```
## Description of dataset columns
```{r, display column names, echo=FALSE}
tbl <- matrix(c(1:36), ncol = 3, byrow= TRUE)
rownames <- c("Identification", "Gender", "Age (in years)", "Driver's License Nbr", "Region Code", "Previously Insured", "Insured Vehicles Age", "Vehicle Damage",
             "Annual Premium", "Policy Sales Channel", "Vintage", "Response")
colnames <- c("Name", "Description", "Class")

col3 <- vector()
for (col in 1:ncol(train_data)) {col3 <- c(col3, class(train_data[,col]))}
colnames(tbl) <- colnames
rownames(tbl) <- rownames
tbl[,1] <- as.vector(colnames(train_data))
tbl[,2] <- c("id", "Gender", "Age", "Driving_License", "Region_Code", "Previously_Insured", "Vehicle's_age", "Vehicle_damage", "Annual Premium", "Policy_Sales_Channel",
          "Vintage", "Success")
tbl[,3] <- col3
knitr::kable(tbl, caption = "Column Names in Dataset", align="c")
```

### Data Preparation
Data preparation and cleaning are done by a function so that it can be reused for different test and training data sets.

    * The function provides the following operations list
        + removal of any NAs 
        + removal of any rows that contain blanks 
        + checks near zero variance and removes any columns that have a near zero variance 
        + converts the Gender column to numeric 1 or 0 
        + converts the Vehicle_Damage column to numeric 
        + converts the vehicle_Age column to a numerical value of the unique values in the column 

```{r data prep via function}
train_data <- data_prep(train_data)
```
This shows that Driving_License and annual premium are not going to be a good predictor so we should remove them from both the training and the testing datasets.


## Preprocessing
One might suspect that there would be no positive responses when there was no previous insurance applied. To determine if this is true we look at the number of positive responses when the column *Previously_insured* is 0.
```{r previousl;y_insured}
num <- sum(train_data$Previously_Insured == 0 & train_data$Response == TRUE)
num
```
This shows that our suspicion is not valid and we need to consider *Previously_insured* as a feature.


We now consider the *Response* column. What percentage of the reponses are positive, that is, how many cross-selling attempts were successful. We then ask the question if there are any vehicles that are not insured that have a positive response.

```{r response mean}
response_mean <- mean(train_data$Response)
plot_df <- data.frame(c("negative", "positive"), c(1-response_mean, response_mean))
colnames(plot_df) <- c("response","percent")

ggplot(data=plot_df) + geom_bar(mapping = aes(x=response, y=percent, fill=response), show.legend=FALSE, stat="identity")

```
The graph shows that there is a definite component of positive response from customers that have not purchased insurance before.

The training dataset contains over 11 million rows while the test dataset contains over 7 million rows. We will be developing this analysis on a small laptop and
want to keep the the run time to be somewhat reasonable, so we will build subsets that will contain `r NUMBER_OF_TRAINING_ROWS` and `r NUMBER_OF_TEST_ROWS` records for the training and testing datasets respectively. There will be a total of `r NUMBER_OF_TEST_SETS` testing datasets sampled from the entire training set. When training and evaluating of the various models, the same datasets will be used.



```{r build subset, echo=FALSE, error=FALSE}
set.seed(2024)
train_set <- sample_n(train_data, NUMBER_OF_TRAINING_ROWS)
train_set_x <- train_set %>% select(-Response)
train_set_y <- as.factor(train_set$Response)
test_sets <- list()
for (i in 1:NUMBER_OF_TEST_SETS) {
  test_set  <- sample_n(train_data, NUMER_OF_TEST_ROWS)
  test_sets <- list.append(test_sets, test_set)
}

```

We use the k-NN method of training hich has a parameter of k. We examine the accuracy of the training for various values of k. We initially 
try a k in the set of `r seq(3, 11, 2)`

```{r first set of training, echo=FALSE}
k_hist <- list()
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn1 <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = c(seq(3,11,2))), trControl = control)

df_knn1 <- train_knn1$results     # extract results as a data frame
k_hist <- append(k_hist, df_knn1)

max_accuracy <- which.max(df_knn1$Accuracy)
max_k <- df_knn1$k[max_accuracy]

a <- max_k - 2
b <- max_k + 2
if (a < 1) a <- 1

range <- seq(a,b)

```

```{r second set of training, echo = FALSE}

train_knn2 <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = range), trControl = control)

df_knn2 <- train_knn2$results
max_accuracy <- which.max(df_knn2$Accuracy)
max_k <- df_knn2$k[max_accuracy]

df_knn <- merge(df_knn1, df_knn2, by=c("Accuracy","k", "Kappa", "AccuracySD", "KappaSD"), all=TRUE)
```
From this second set of iterations, the maximum k value that we should use is `r max_k`. The final algorithm is calculated with this value.

```{r final train, echo=FALSE}
train_knn <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = max_k), trControl = control)

fit_knn <- knn3(train_set_x, train_set_y, k=max_k)


```


Now consider the test set that we carved from the original data set. How does it match with the algorithm (function *predict*)?

```{r test_sets, echo=FALSE}
accum_accuracy <- 0
knn_fcn <- function(test_set) {
  test_set_y <- as.factor(test_set$Response)
  test_set_x <- test_set %>% select(-Response)
  y_hat <- predict(fit_knn, test_set_x, type = "class")
  cm <- confusionMatrix(y_hat, factor(test_set_y))
  print(paste("Accuracy: ", cm$overall["Accuracy"], 
              "Sensitivity: ", cm$byClass["Sensitivity"],
              "Specificity: ", cm$byClass["Specificity"]))
  accum_accuracy <<- accum_accuracy + cm$overall["Accuracy"]
}

lapply(test_sets, knn_fcn)
print(paste("k-NN accuracy: ", as.character(accum_accuracy / NUMBER_OF_TEST_SETS)))

```

This algorithm ws based on the k-NN (nearest neighbor). Other models should be tested for comparisons. The first alternative to attempt is random forest, which is a compute-intensive model. There are several parameters that can be tuned to adjust the model and we will need to built a multitude of trees. As a result of this, we will only use a five-fold cross validation.

```{r random forest, echo = FALSE}
accum_accuracy <- 0
rf_fcn <- function(test_set) {
  test_set_y <- as.factor(test_set$Response)
  test_set_x <- test_set %>% select(-Response)
  y_hat <- predict(fit_rf, test_set_x, type = "class")
  cm <- confusionMatrix(y_hat, factor(test_set_y))
  print(paste("Accuracy: ", cm$overall["Accuracy"], 
              "Sensitivity: ", cm$byClass["Sensitivity"],
              "Specificity: ", cm$byClass["Specificity"]))
  accum_accuracy <<- accum_accuracy + cm$overall["Accuracy"]
}

test_set_x <- test_set %>% select(-Response)
test_set_y <- as.factor(test_set$Response)
control <- trainControl(method = "cv", number = 5)
grid <- data.frame(mtry = c(1,2,3))
train_rf <- train(train_set_x, train_set_y, method="rf", ntree = 500, trControl = control, tuneGrid = grid, nSamp = 10000)
fit_rf <- randomForest(train_set_x, train_set_y, mtry = train_rf$bestTune$mtry)
y_hat_rf <- predict(fit_rf, train_set_x, type="class")
cm <- confusionMatrix(y_hat_rf, factor(train_set_y))
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]


lapply(test_sets, rf_fcn)

print(paste("random forest accuracy:", as.character(accum_accuracy / 10)))


```
The average of these 10 test cases is `r accum_accuracy`. We can see that the most important feature is whether the vehicle has been damaged followe closelyby whether the customer has been previous insured.
```{r graph importance}
importance <- fit_rf$importance
colnames(importance) <- c("Mean")
sorted_importance <- as.data.frame(importance[order(importance[,"Mean"]),]) * 100/ sum(importance)
colnames(sorted_importance) <- c("sorted_importance")


xlabls <- gsub("_", " ", rownames(sorted_importance))
ggplot(data=sorted_importance, aes(x = reorder(xlabls, -sorted_importance), y=sorted_importance, fill=reorder(xlabls, -sorted_importance))) + 
  geom_bar(show.legend=FALSE, stat="identity") +
  xlab("Feature") +
  ylab("Percent") +
  ylim(0, 50) +
  ggtitle("Relative Importance") +
  theme(plot.title = element_text(hjust = 0.5, size = 9)) +
  guides(x = guide_axis(angle = 90))  
```
    
### Ensemble
We can combine the two models that we have tried (knn and random forest) to determine if the combination is better than either one. 
```{r ensemble, echo=FALSE}
ensemble_fcn <- function(test_set) {
  test_set_y <- as.factor(test_set$Response)
  test_set_x <- test_set %>% select(-Response)
  
  p_rf <- predict(fit_rf, test_set_x, type = "prob")
  p_rf <- p_rf / rowSums(p_rf)
  p_knn <- predict(fit_knn, test_set_x)
  p <- (p_rf + p_knn) / 2
  y_pred <- factor(apply(p, 1, which.max)-1)
  accum_accuracy <<- accum_accuracy + confusionMatrix(y_pred, test_set_y)$overall["Accuracy"]
  
}
accum_accuracy <- 0
lapply(test_sets, ensemble_fcn)
print(paste("Ensemble accuracy: ", as.character(accum_accuracy / 10)))


```

### Other models (taking defaults)
The end


### Conclusion
TBD

'''

train_set_x <- train_set %>% select(-Response)
train_set_y <- as.factor(train_set$Response)

fit_model <- kknn(train_set_x, train_set_y)

y_hat_model <- predict(fit_model, train_set_x, type="class")
cm <- confusionMatrix(y_hat_model, factor(test_set_y))
print(paste("Accuracy: ", cm$overall["Accuracy"], 
              "Sensitivity: ", cm$byClass["Sensitivity"],
              "Specificity: ", cm$byClass["Specificity"]))



'''