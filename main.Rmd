---
title: "Insurance Cross Selling (Kaggle)"
author: "Ellis Hodgdon"
date: "2024-08-08"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir ="c:/users/ehodgdon/OneDrive/HarvardX/Data_Science/DataScienceProjects/DataScience/")
knitr::opts_chunk$set(warning=FALSE)
```
### Introduction, setup, loading necessary libraries, and adjust working directories                 

```{r load libraries, echo=FALSE, message=FALSE, warning = FALSE}

library(tidyverse, quietly = TRUE, warn.conflicts=FALSE)
  
if (!require(rvest)) install.packages("rvest", verbose=FALSE)
if (!require(dplyr)) instsall.packages("dplyr", verbose=FALSE)
if (!require(tidyr)) install.packages("tidyr", verbose=FALSE)
if (!require(caret)) install.packages("caret", verbose=FALSE)

path <- "c:/users/ehodgdon/OneDrive/HarvardX/Data_Science/DataScienceProjects/DataScience/"
setwd(path)
```
    The libraries that were installed (if needed) and loaded were:
    
    * tidyverse
    * rvest
    * dplyr
    * tidyr
    * caret

```{r working directory}
print(paste("The current working diretory is ", getwd()))
```

  
### Problem Definition
The goal of this analysis is to predict which customers respond positively to an automobile insurance offer based on multiple features such as gender, age, the age of the vehicle, if the vehicle had been damaged, etc. During the exploratory data analysis we will determine which features are not important and will not include these in our training for the algorithm.

The column *Result* as integer (0 or 1) indicates whether a resale was successful with this customer.


### Input the training dataset and data prep
Kaggle provides two datasets -- training and test -- but the test dataset is only used to judge the developed algorithm. It is missing the *Response* column.
We will do some data cleaning on the training dataset but since it contains over 11 million records, we take a subset of the main dataset and divide that into into two smaller datasets that we can reasonably handle. We will work with a training dataset of 50,000 records and a testing dataset of 5000 records.
```{r read datasets, echo = FALSE}
train_data <- read.csv("insurance-train.csv")


data_prep <- function(df, nzv_arg = FALSE, response = TRUE) {
  # check for any rows containing NAs
  na_counts <- train_data %>% summarise_all(~ sum(is.na(.)))
  number_of_NAs <- sum(na_counts)
  if (number_of_NAs > 0 ) {
    df <- na.omit(df)
    number_of_NAs
  }
   # A value of zero indicates there are no NAs in the entire dataset. Next, check of missing data

   # Check for any rows containing blanks
  na_nulls <- sum(is.null(train_data))
  number_of_nulls <- sum(na_nulls)
  if (number_of_nulls > 0) {
    df[df == "NULL"] <- NA
    df <- na.omit(df)
    number_of_nulls
  }

  # Using the nearZeroVar() function from the caret package,we can determine which predictors are near zero variance and therefore would not be a
  # good predictor.
  ifelse (!nzv_arg,   nzv_list <<- nearZeroVar(train_data), nzv_list <- nzv_arg)    # <<- global assignment operator
  if (length(nzv_list) > 0) {
    df <- df[, -nzv_list]  # remove near zero variances
    for (i in nzv_list) print(colnames(train_data)[i])
  }    # end of if (length(nzv_list) ...
  # This shows that Driving_License and annual premium are not going to be a good predictor so we should remove them from both the training and the   # testing datasets.
  
  # convert the Gender column from "Male", "Female" to 0 and 1df
  df$Gender <- ifelse(df$Gender == "Male", 0, 1)
  
  # convert the Vehicle_Damage column from "Yes" and "No" ti 1 and 0
  df$Vehicle_Damage <- ifelse(df$Vehicle_Damage == "Yes", 1, 0)
  
  # convert the Vehicle_Age from a text to a numeric value
  unique <- unique(train_data$Vehicle_Age)
  for (age in unique) {df$Vehicle_Age[df$Vehicle_Age == age] <- which(unique == age)}
  
  #At this point all values in the data frame are numeric


 return (df)
}   # end of function

```
### Description of dataset columns
```{r, display column names, echo=FALSE}

tbl <- matrix(c(1:36), ncol = 3, byrow= TRUE)
rownames <- c("Identification", "Gender", "Age (in years)", "Driver's License Nbr", "Region Code", "Previously Insured", "Insured Vehicles Age", "Vehicle Damage",
             "Annual Premium", "Policy Sales Channel", "Vintage", "Response")
colnames <- c("Name", "Description", "Class")

col3 <- vector()
for (col in 1:ncol(train_data)) {col3 <- c(col3, class(train_data[,col]))}
colnames(tbl) <- colnames
rownames(tbl) <- rownames
tbl[,1] <- as.vector(colnames(train_data))
tbl[,2] <- c("id", "Gender", "Age", "Driving_License", "Region_Code", "Previously_Insured", "Vehicle's_age", "Vehicle_damage", "Annual Premium", "Policy_Sales_Channel",
          "Vintage", "Success")
tbl[,3] <- col3
knitr::kable(tbl, caption = "Column Names in Dataset", align=rep('l', 5)) 
```

## Data Preparation
Data preparation and cleaning are done by a function so that it can easily be reused for different test and training data sets.

    * The function provides the following operations
        + removal of any NAs 
        + removal of any rows that contain blanks 
        + checks near zero variance and removes any columns that have a near zero variance 
        + converts the Gender column to numeric 1 or 0 
        + converts the Vehicle_Damage column to numeric 
        + converts the vehicle_Age column to a numerical value of the unique values in the column 

```{r data prep via function}
train_data <- data_prep(train_data)
```


## Preprocessing
One might suspect that there would be no positive responses when there was no previous insurance applied. To determine if this is true we look at the number of positive responses when the column *Previously_insured* is 0.

```{r previousl;y_insured}
num <- sum(train_data$Previously_Insured == 0 & train_data$Response == TRUE)
num
```
This shows that our suspicion is not valid and we need to consider *Previously_insured* as a feature.

We check the variances of the various columns and determine that Driving_License and annual premium are not going to be a good predictor so we should remove them from both the training and the testing datasets.


We now consider the *Response* column. What percentage of the reponses are positive, that is, how many cross-selling attempts were successful. We then ask the question if there are any vehicles that are not insured that have a positive response.

```{r response mean}
response_mean <- mean(train_data$Response)
plot_df <- data.frame(c("positive", "negative"), c(1-response_mean, response_mean))
colnames(plot_df) <- c("response","percent")

# hist(c(1*response_mean, 1-1*response_mean), col=c('red','blue'), main="Response")
# uninsured_positives <- sum(train_data$response == 0)
ggplot(data=plot_df) + geom_bar(mapping = aes(x=response, y=percent, fill=response), show.legend=FALSE, stat="identity")

```
  
The training dataset contains over 11 million rows while the test dataset contains over 7 million rows. We will developing this analysis on a small laptop and
want to keep the the run time to be somewhat reasonable, so we'll build subsets that will contain 10,000 and 1,000 records for the training and testing datasets respectively.



```{r build subset, echo=FALSE}
set.seed(2024)
test_set <- sample_n(train_data, 5000)
train_set <- sample_n(train_data, 50000)
train_set_x <- train_set[,-train_set$Response]
train_set_y <- as.factor(train_set$Response)


start.time <- Sys.time()
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn1 <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = c(seq(3,11,2))), trControl = control)
end.time <- Sys.time()
time.taken <- round(end.time - start.time)
time.taken

df_knn1 <- train_knn1$results

max_accuracy <- which.max(df_knn1$Accuracy)
max_k <- df_knn1$k[max_accuracy]

a <- max_k - 2
b <- max_k + 2
if (a < 1) a <- 1

range <- seq(a,b)
range

train_knn2 <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = range), trControl = control)

df_knn2 <- train_knn2$results
max_accuracy <- which.max(df_knn2$Accuracy)
max_k <- df_knn2$k[max_accuracy]

df_knn <- merge(df_knn1, df_knn2, by=c("Accuracy","k", "Kappa", "AccuracySD", "KappaSD"), all=TRUE)
k_max <- which.max(df_knn$Accuracy)

train_knn <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = k_max), trControl = control)

fit_knn <- knn3(train_set_x, train_set_y, k=9)

y_hat_knn <- predict(fit_knn, train_set_x, type="class")
cm <- confusionMatrix(y_hat_knn, factor(train_set_y))
cm
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]

```


```
Now consider the test set that we carved from the original data set. How does it match with the algorithm (function *predict*)?

```{r test_set, echo=FALSE}
test_set_x <- test_set[,-train_set$Response]
test_set_y <- as.factor(test_set$Response)

test_y_hat_knn <- predict(fit_knn, test_set_x, type="class")
test_cm <- confusionMatrix(test_y_hat_knn, factor(test_set_y))
test_cm
test_cm$overall["Accuracy"]
test_cm$byClass["Sensitivity"]
test_cm$byClass["Specificity"]
```

