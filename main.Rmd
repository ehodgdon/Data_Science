<<<<<<< HEAD
---
title: "Insurance Cross Selling (Kaggle)"
author: "Ellis Hodgdon"
date: "2024-08-08"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning=FALSE)
```

### Introduction, setup, loading necessary libraries, and adjust working directories   
```{r}
NUMBER_OF_TEST_SETS = 10
NUMBER_OF_TRAINING_ROWS = 50000
NUMBER_OF_TEST_ROWS = 5000

results <- data.frame()    # for the summary of results


```


```{r load libraries, echo=FALSE, include=FALSE}
if (!require(tidyverse)) suppressMessages(install.packages(tidyverse, verbose=FALSE, quiet = TRUE))
if (!require(rvest)) install.packages("rvest", verbose=FALSE)
if (!require(dplyr)) instsall.packages("dplyr", verbose=FALSE)
if (!require(tidyr)) install.packages("tidyr", verbose=FALSE)
if (!require(caret)) install.packages("caret", verbose=FALSE)
if (!require(caret)) install.packages("caret", verbose=FALSE)
if (!require(rlist)) suppressMessages(install.packages("rlist", verbose=FALSE))
if (!require(randomForest)) suppressMessages(install.packages("randomForest", verbose=FALSE))

```
```{r function definitions, echo = FALSE, include=FALSE}
data_prep <- function(df, nzv_arg = FALSE, response = TRUE) {
  # check for any rows containing NAs
  na_counts <- train_data %>% summarise_all(~ sum(is.na(.)))
  number_of_NAs <- sum(na_counts)
  if (number_of_NAs > 0 ) {
    df <- na.omit(df)
    number_of_NAs
  }
   # A value of zero indicates there are no NAs in the entire dataset. Next, check of missing data

   # Check for any rows containing blanks
  na_nulls <- sum(is.null(train_data))
  number_of_nulls <- sum(na_nulls)
  if (number_of_nulls > 0) {
    df[df == "NULL"] <- NA
    df <- na.omit(df)
    number_of_nulls
  }

  # Using the nearZeroVar() function from the caret package,we can determine which predictors are near zero variance and therefore would not be a
  # good predictor.
  ifelse (!nzv_arg,   train_nzv <- nearZeroVar(train_data), train_nzv <- nzv_arg)
  if (length(train_nzv) > 0) {
    for (i in train_nzv) {
    print(paste('***', i, '   ', colnames(train_data)[i]))
    df <- df[, -train_nzv]  # remove near zero variances
    }
  }  # end of if (length(nzv) ...
   
  
  # convert the Gender column from "Male", "Female" to 0 and 1df
  df$Gender <- ifelse(df$Gender == "Male", 0, 1)
  
  # convert the Vehicle_Damage column from "Yes" and "No" ti 1 and 0
  df$Vehicle_Damage <- ifelse(df$Vehicle_Damage == "Yes", 1, 0)
  
  # convert the Vehicle_Age from a text to a numeric value
  unique <- unique(train_data$Vehicle_Age)
  for (age in unique) {df$Vehicle_Age[df$Vehicle_Age == age] <- which(unique == age)}
  
  #At this point all values in the data frame are numeric


 return (df)
}      # end of data_prep function

  
knn_fcn <- function(test_set) {
  df <- as.data.frame(test_set)
  test_set_y <- df$Response
  test_set_x <- test_set %>% select(-Response)
  y_hat <- predict(fit_knn, test_set_x, type = "class")
  cm <- confusionMatrix(y_hat, factor(test_set_y))
  # print(paste("Accuracy: ", cm$overall["Accuracy"], 
  #             "Sensitivity: ", cm$byClass["Sensitivity"],
  #             "Specificity: ", cm$byClass["Specificity"]))
  results <<- rbind(results, list("k-NN", cm$overall["Accuracy"], cm$byClass["Sensitivity"], cm$byClass["Specificity"]))
  accum_accuracy <<- accum_accuracy + cm$overall["Accuracy"]
  return
}


rf_fcn <- function(test_set) {
  test_set_y <- as.factor(test_set$Response)
  test_set_x <- test_set %>% select(-Response)
  y_hat <- predict(fit_rf, test_set_x, type = "class")
  cm <- confusionMatrix(y_hat, factor(test_set_y))
  print(paste("Accuracy: ", cm$overall["Accuracy"], 
              "Sensitivity: ", cm$byClass["Sensitivity"],
              "Specificity: ", cm$byClass["Specificity"]))
results <<- rbind(results, list("rf", cm$overall["Accuracy"], cm$byClass["Sensitivity"], cm$byClass["Specificity"]))
accum_accuracy <<- accum_accuracy + cm$overall["Accuracy"]
return 
}



glm_model <- function(dp, fam = gaussian()) {
model <- glm(Response ~ Previously_Insured + Vehicle_Age + Vehicle_Damage + Vehicle_Age, 
                family = fam, data = dp)
summary(model)
y_hat <- predict(model, dp %>% select(-Response), type="response")
cutoff <- quantile(y_hat, 0.18)
pred_glm <- ifelse(y_hat < 0.0006104226, "1", "0")
pred_glm = as.factor(pred_glm)

cm <- confusionMatrix(as.factor(dp$Response), pred_glm) 
results <<- rbind(results, list("glm", cm$overall["Accuracy"], cm$byClass["Sensitivity"], cm$byClass["Specificity"]))

return 
}


ensemble_fcn <- function(test_set) {
  test_set_y <- as.factor(test_set$Response)
  test_set_x <- test_set %>% select(-Response)
  
  p_rf <- predict(fit_rf, test_set_x, type = "prob")
  p_rf <- p_rf / rowSums(p_rf)
  p_knn <- predict(fit_knn, test_set_x)
  p <- (p_rf + p_knn) / 2
  y_pred <- factor(apply(p, 1, which.max)-1)
  cm <- confusionMatrix(y_pred, test_set_y)
  accum_accuracy <<- accum_accuracy + cm$overall["Accuracy"]
  results <<- rbind(results, list('ensemble', cm$overall["Accuracy"], cm$byClass["Sensitivity"], cm$byClass["Specificity"]))
  return
}


n_bayes <- function(dp) {
  dp_y <- as.factor(dp$Response)
  dp_x <- dp %>% select(-Response)

  bayes <- train(as.factor(Response) ~ Vehicle_Damage + Previously_Insured + Age + Vehicle_Age, method="naive_bayes", data = dp, usepoisson = TRUE)
  y_hat <- predict(bayes, dp_x)
  cm <- confusionMatrix(y_hat, dp_y)
  results <<- rbind(results, list("naive Bayes",cm$overall["Accuracy"], cm$byClass["Sensitivity"], cm$byClass["Specificity"]))
  return 
}

```



### Problem Definition
The goal of this analysis is to predict which customers respond positively to an automobile insurance offer.

The column *Result* as integer (0 or 1) indicates whether a resale was successful with this customer.


### Input the training dataset and data prep
Kaggle provides two datasets -- training and test -- but the test dataset is only used to judge the developed algorithm. It is missing the *Response* column.We will do some work on the entire training dataset and then split it into smaller datasets that we can reasonably handle.
```{r read datasets, echo = FALSE}

train_data <- read.csv("insurance-train.csv")
# train_data <- read.csv("small_insurance_train.csv")

 
```
### Description of dataset columns
```{r, display column names, echo=FALSE}
tbl <- matrix(c(1:36), ncol = 3, byrow= TRUE)
rownames <- c("Identification", "Gender", "Age (in years)", "Driver's License Nbr", "Region Code", "Previously Insured", "Insured Vehicles Age", "Vehicle Damage",
             "Annual Premium", "Policy Sales Channel", "Vintage", "Response")
colnames <- c("Name", "Description", "Class")

col3 <- vector()
for (col in 1:ncol(train_data)) {col3 <- c(col3, class(train_data[,col]))}
colnames(tbl) <- colnames
rownames(tbl) <- rownames
tbl[,1] <- as.vector(colnames(train_data))
tbl[,2] <- c("id", "Gender", "Age", "Driving_License", "Region_Code", "Previously_Insured", "Vehicle's_age", "Vehicle_damage", "Annual Premium", "Policy_Sales_Channel",
          "Vintage", "Success")
tbl[,3] <- col3
knitr::kable(tbl, caption = "Column Names in Dataset", align="c")
```

### Data Preparation
Data preparation and cleaning are done by a function so that it can be reused for different test and training data sets, if necessary.

    * The function provides the following operations
        + removal of any NAs 
        + removal of any rows that contain blanks 
        + checks near zero variance and removes any columns that have a near zero variance 
        + converts the Gender column to numeric 1 or 0 
        + converts the Vehicle_Damage column to numeric 
        + converts the vehicle_Age column to a numerical value of the unique values in the column 

```{r data prep via function}
train_data <- data_prep(train_data)
```
This shows that Driving_License and Annual Premium are not going to be a good predictor so we should remove them from both the training and the testing datasets.


## Preprocessing
One might suspect that there would be no positive responses when there was no previous insurance applied. To determine if this is true we look at the number of positive responses when the column *Previously_Insured* is 0.
```{r previousl;y_insured}
num <- sum(train_data$Previously_Insured == 0 & train_data$Response == TRUE)
num
```
This shows that our suspicion is not valid and we need to consider *Previously_insured* as a feature.


We now consider the *Response* column. What percentage of the reponses are positive, that is, how many cross-selling attempts were successful. We then ask the question if there are any vehicles that are not insured that have a positive response.

```{r response mean, fig.align="center", echo=FALSE, fig.width = 5, fig.height= 2.5}
response_mean <- mean(train_data$Response)
plot_df <- data.frame(c("negative", "positive"), c(1-response_mean, response_mean))
colnames(plot_df) <- c("response","percent")

ggplot(data=plot_df) + geom_bar(mapping = aes(x=response, y=percent, fill=response), show.legend=FALSE, stat="identity")    

```
The graph shows that there is a definite component of positive response from customers that have not purchased insurance before.

The training dataset contains over 11 million rows while the test dataset contains over 7 million rows. We will be developing this analysis on a small laptop and
want to keep the the run time to be somewhat reasonable, so we will build subsets that will contain `r NUMBER_OF_TRAINING_ROWS` and `r NUMBER_OF_TEST_ROWS` records for the training and testing datasets respectively. There will be a total of `r NUMBER_OF_TEST_SETS` testing datasets sampled from the entire training set. When training and evaluating of the various models, the same datasets will be used.



```{r build subset, echo=FALSE, error=FALSE}
set.seed(2024)
train_set <- sample_n(train_data, NUMBER_OF_TRAINING_ROWS)
train_set_x <- train_set %>% select(-Response)
train_set_y <- as.factor(train_set$Response)
test_sets <- list()
for (i in 1:NUMBER_OF_TEST_SETS) {
  test_set  <- sample_n(train_data, NUMBER_OF_TEST_ROWS)
  test_sets <- list.append(test_sets, test_set)
}

```

We use the k-NN method of training which has a parameter of k. We examine the accuracy of the training for various values of k. We initially 
try a k in the set of `r seq(3, 11, 2)`

```{r first set of training, echo=FALSE}
k_hist <- list()
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn1 <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = c(seq(3,11,2),20)), trControl = control)

df_knn1 <- train_knn1$results     # extract results as a data frame
k_hist <- list.append(k_hist, df_knn1)

max_accuracy <- which.max(df_knn1$Accuracy)
max_k <- df_knn1$k[max_accuracy]

a <- max_k - 2
b <- max_k + 2
if (a < 1) a <- 1

# range <- seq(a,b)

```

```{r second set of training, echo = FALSE, include = FALSE}

train_knn2 <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = seq(a,b)), trControl = control)

df_knn2 <- train_knn2$results
max_accuracy <- which.max(df_knn2$Accuracy)
max_k <- df_knn2$k[max_accuracy]

df_knn <- merge(df_knn1, df_knn2, by=c("Accuracy","k", "Kappa", "AccuracySD", "KappaSD"), all=TRUE)
```
```{r graghing value of k, echo=FALSE, fig.align = "center", fig.height = 4, fig.width = 5}
all_k <- full_join(df_knn1, df_knn2)
sorted_k <- all_k[order(all_k$k),]
ggplot(sorted_k, aes(x=k, y=Accuracy))+ geom_line()
max_k <- which.max(sorted_k$Accuracy)


```

From this second set of iterations, the maximum k value that we should use is `r max_k`. The final algorithm is calculated with this value.

```{r final train, echo=FALSE}
train_knn <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = max_k), trControl = control)

fit_knn <- knn3(train_set_x, train_set_y, k=max_k)

```
The system picked a value of `r max_k` for our calculations. Let's examine the derivation of this number




Now consider the test set that we carved from the original data set. How does it match with the algorithm (function *predict*)? Here are the accuracy results for the k-NN model:

```{r test_sets, echo=FALSE, include=FALSE}

accum_accuracy <- 0
lapply(test_sets, knn_fcn)
print(paste("k-NN accuracy: ", as.character(accum_accuracy / NUMBER_OF_TEST_SETS)))
# results[[nrow(results), 1]] <- paste(results[[nrow(results), 1]], "( k=", max_k, ")")

```

This algorithm ws based on the k-NN (nearest neighbor). Other models should be tested for comparisons. The first alternative to attempt is random forest, which is a compute-intensive model. There are several parameters that can be tuned to adjust the model and we will need to built a multitude of trees. As a result of this, we will only use a five-fold cross validation.

```{r random forest, echo = FALSE, error = FALSE, include=FALSE}
accum_accuracy <- 0

test_set_x <- test_set %>% select(-Response)
test_set_y <- as.factor(test_set$Response)
control <- trainControl(method = "cv", number = 5)
grid <- data.frame(mtry = c(1,2,3))
train_rf <- train(train_set_x, train_set_y, method="rf", ntree = 500, trControl = control, tuneGrid = grid, nSamp = 10000)
fit_rf <- randomForest(train_set_x, train_set_y, mtry = train_rf$bestTune$mtry)
y_hat_rf <- predict(fit_rf, train_set_x, type="class")
cm <- confusionMatrix(y_hat_rf, factor(train_set_y))
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]


lapply(test_sets, rf_fcn)



```
The average of these 10 test cases is `r accum_accuracy`. We can see that the most important feature is whether the vehicle has been damaged followe closelyby whether the customer has been previous insured.
```{r graph importance, echo=FALSE, fig.align = "center"}
importance <- fit_rf$importance
colnames(importance) <- c("Mean")
sorted_importance <- as.data.frame(importance[order(importance[,"Mean"]),]) * 100/ sum(importance)
colnames(sorted_importance) <- c("sorted_importance")


xlabls <- gsub("_", " ", rownames(sorted_importance))
ggplot(data=sorted_importance, aes(x = reorder(xlabls, -sorted_importance), y=sorted_importance, fill=reorder(xlabls, -sorted_importance))) + 
  geom_bar(show.legend=FALSE, stat="identity") +
  xlab("Feature") +
  ylab("Percent") +
  ylim(0, 50) +
  ggtitle("Relative Importance") +
  theme(plot.title = element_text(hjust = 0.5, size = 9)) +
  guides(x = guide_axis(angle = 90))  
```
    
### Ensemble
```{r ensemble, echo=FALSE, include=FALSE}
accum_accuracy <- 0
lapply(test_sets, ensemble_fcn)


```
When we combine the two models that we have tried (knn and random forest) to determine if the combination is better than either one. However, the ensemble accuracy turns out to be `r as.character(accum_accuracy / 10)` which is below the random forest accuracy of ?????


### Other models (taking defaults)


```{r glm test cases, echo=FALSE, include = FALSE}
glm_model(test_set, binomial)
results[[nrow(results),1]] <- paste(results[[nrow(results),1]],  "(binomial)")

glm_model(test_set, gaussian())
results[[nrow(results),1]] <- paste(results[[nrow(results),1]],  "(gaussian)")

glm_model(test_set, quasi(link = "identity", variance ="constant"))
results[[nrow(results),1]] <- paste(results[[nrow(results),1]],  "(quasi)")

glm_model(test_set, quasibinomial(link = "logit"))
results[[nrow(results),1]] <- paste(results[[nrow(results),1]],  "(quasibinomial)")

```
We've used two of the over 21,000 models that are available to determine from CRAN to determine a best fit for our data. The first model that was investigated was *glm* or General Linear Model, which is a generalization of ordinary linear regression but accommodates distributions other than normal to include poisson, gaussian, binomial, multinominal, quasi, quasibinomial and gamma. Four of the families (binomial, gaussian, quasi and quasibinomial) produced the same accuracy result of `r results[[nrow(results),2]]`.


Another model that was tried was the *naive Bayes* model which seeks the model the distribution of inputs but does not learn which features are most important. Again, using the standard train set, we find that the accuracy from the confusion matrix is 0.7121, which is still lower that was obtained from either the *k-NN* model or the *random forest* model.

```{r naive_bayes model testing, echo=FALSE, include=FALSE}
n_bayes(train_set)

```
### Conclusion
```{r summary table, echo=FALSE}
colnames(results) <- c("Model", "Accuracy", "Sensitivity", "Specificity")
table_results <- results[order(-results$Accuracy),]
table_results <- table_results %>% distinct(Model, .keep_all=TRUE)
colnames(table_results) <- c("Model", "Accuracy", "Sensitivity", "Specificity")

knitr::kable(table_results, caption = "Summary of Models", digits = 4)
```



That's all folks!

=======
---
title: "Insurance Cross Selling (Kaggle)"
author: "Ellis Hodgdon"
date: "2024-08-08"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning=FALSE)
```

### Introduction, setup, loading necessary libraries, and adjust working directories   
```{r}
NUMBER_OF_TEST_SETS = 10
NUMBER_OF_TRAINING_ROWS = 50000
NUMBER_OF_TEST_ROWS = 5000

```


```{r load libraries, echo=FALSE, include=FALSE}
if (!require(tidyverse)) suppressMessages(install.packages(tidyverse, verbose=FALSE, quiet = TRUE))
if (!require(rvest)) install.packages("rvest", verbose=FALSE)
if (!require(dplyr)) instsall.packages("dplyr", verbose=FALSE)
if (!require(tidyr)) install.packages("tidyr", verbose=FALSE)
if (!require(caret)) install.packages("caret", verbose=FALSE)
if (!require(caret)) install.packages("caret", verbose=FALSE)
if (!require(rlist)) suppressMessages(install.packages("rlist", verbose=FALSE))
if (!require(randomForest)) suppressMessages(install.packages("randomForest", verbose=FALSE))

```
```{r function definitions}
data_prep <- function(df, nzv_arg = FALSE, response = TRUE) {
  # check for any rows containing NAs
  na_counts <- train_data %>% summarise_all(~ sum(is.na(.)))
  number_of_NAs <- sum(na_counts)
  if (number_of_NAs > 0 ) {
    df <- na.omit(df)
    number_of_NAs
  }
   # A value of zero indicates there are no NAs in the entire dataset. Next, check of missing data

   # Check for any rows containing blanks
  na_nulls <- sum(is.null(train_data))
  number_of_nulls <- sum(na_nulls)
  if (number_of_nulls > 0) {
    df[df == "NULL"] <- NA
    df <- na.omit(df)
    number_of_nulls
  }

  # Using the nearZeroVar() function from the caret package,we can determine which predictors are near zero variance and therefore would not be a
  # good predictor.
  ifelse (!nzv_arg,   train_nzv <- nearZeroVar(train_data), train_nzv <- nzv_arg)
  if (length(train_nzv) > 0) {
    for (i in train_nzv) {
    df <- df[, -train_nzv]  # remove near zero variances
    }
  }  # end of if (length(nzv) ...
   
  
  # convert the Gender column from "Male", "Female" to 0 and 1df
  df$Gender <- ifelse(df$Gender == "Male", 0, 1)
  
  # convert the Vehicle_Damage column from "Yes" and "No" ti 1 and 0
  df$Vehicle_Damage <- ifelse(df$Vehicle_Damage == "Yes", 1, 0)
  
  # convert the Vehicle_Age from a text to a numeric value
  unique <- unique(train_data$Vehicle_Age)
  for (age in unique) {df$Vehicle_Age[df$Vehicle_Age == age] <- which(unique == age)}
  
  #At this point all values in the data frame are numeric


 return (df)
}      # end of data_prep function

  
knn_fcn <- function(test_set) {
  df <- as.data.frame(test_set)
  test_set_y <- df$Response
  test_set_x <- test_set %>% select(-Response)
  y_hat <- predict(fit_knn, test_set_x, type = "class")
  cm <- confusionMatrix(y_hat, factor(test_set_y))
  print(paste("Accuracy: ", cm$overall["Accuracy"], 
              "Sensitivity: ", cm$byClass["Sensitivity"],
              "Specificity: ", cm$byClass["Specificity"]))
  accum_accuracy <<- accum_accuracy + cm$overall["Accuracy"]
}


rf_fcn <- function(test_set) {
  test_set_y <- as.factor(test_set$Response)
  test_set_x <- test_set %>% select(-Response)
  y_hat <- predict(fit_rf, test_set_x, type = "class")
  cm <- confusionMatrix(y_hat, factor(test_set_y))
  print(paste("Accuracy: ", cm$overall["Accuracy"], 
              "Sensitivity: ", cm$byClass["Sensitivity"],
              "Specificity: ", cm$byClass["Specificity"]))
  accum_accuracy <<- accum_accuracy + cm$overall["Accuracy"]
}



glm_model <- function(dp, fam = gaussian()) {
model <- glm(Response ~ Previously_Insured + Vehicle_Age + Vehicle_Damage + Vehicle_Age, 
                family = fam, data = dp)
summary(model)
y_hat <- predict(model, dp %>% select(-Response), type="response")
cutoff <- quantile(y_hat, 0.18)
pred_glm <- ifelse(y_hat < 0.0006104226, "1", "0")
pred_glm = as.factor(pred_glm)

cm <- confusionMatrix(as.factor(dp$Response), pred_glm)
return (cm)
}


n_bayes <- function(dp) {
  dp_y <- as.factor(dp$Response)
  dp_x <- dp %>% select(-Response)

  bayes <- train(as.factor(Response) ~ Vehicle_Damage + Previously_Insured + Age + Vehicle_Age, method="naive_bayes", data = dp, usepoisson = TRUE)
  y_hat <- predict(bayes, dp_x)
  cm <- confusionMatrix(y_hat, dp_y)
  cm
}

```


### Problem Definition
The goal of this analysis is to predict which customers respond positively to an automobile insurance offer.

The column *Result* as integer (0 or 1) indicates whether a resale was successful with this customer.


### Input the training dataset and data prep
Kaggle provides two datasets -- training and test -- but the test dataset is only used to judge the developed algorithm. It is missing the *Response* column.We will do some work on the entire training dataset and then split it into smaller datasets that we can reasonably handle.
```{r read datasets, echo = FALSE}

train_data <- read.csv("insurance-train.csv")
# train_data <- read.csv("small_insurance_train.csv")

 
```
### Description of dataset columns
```{r, display column names, echo=FALSE}
tbl <- matrix(c(1:36), ncol = 3, byrow= TRUE)
rownames <- c("Identification", "Gender", "Age (in years)", "Driver's License Nbr", "Region Code", "Previously Insured", "Insured Vehicles Age", "Vehicle Damage",
             "Annual Premium", "Policy Sales Channel", "Vintage", "Response")
colnames <- c("Name", "Description", "Class")

col3 <- vector()
for (col in 1:ncol(train_data)) {col3 <- c(col3, class(train_data[,col]))}
colnames(tbl) <- colnames
rownames(tbl) <- rownames
tbl[,1] <- as.vector(colnames(train_data))
tbl[,2] <- c("id", "Gender", "Age", "Driving_License", "Region_Code", "Previously_Insured", "Vehicle's_age", "Vehicle_damage", "Annual Premium", "Policy_Sales_Channel",
          "Vintage", "Success")
tbl[,3] <- col3
knitr::kable(tbl, caption = "Column Names in Dataset", align="c")
```

### Data Preparation
Data preparation and cleaning are done by a function so that it can be reused for different test and training data sets, if necessary.

    * The function provides the following operations
        + removal of any NAs 
        + removal of any rows that contain blanks 
        + checks near zero variance and removes any columns that have a near zero variance 
        + converts the Gender column to numeric 1 or 0 
        + converts the Vehicle_Damage column to numeric 
        + converts the vehicle_Age column to a numerical value of the unique values in the column 

```{r data prep via function}
train_data <- data_prep(train_data)
```
This shows that Driving_License and Annual Premium are not going to be a good predictor so we should remove them from both the training and the testing datasets.


## Preprocessing
One might suspect that there would be no positive responses when there was no previous insurance applied. To determine if this is true we look at the number of positive responses when the column *Previously_Insured* is 0.
```{r previousl;y_insured}
num <- sum(train_data$Previously_Insured == 0 & train_data$Response == TRUE)
num
```
This shows that our suspicion is not valid and we need to consider *Previously_insured* as a feature.


We now consider the *Response* column. What percentage of the reponses are positive, that is, how many cross-selling attempts were successful. We then ask the question if there are any vehicles that are not insured that have a positive response.

```{r response mean}
response_mean <- mean(train_data$Response)
plot_df <- data.frame(c("negative", "positive"), c(1-response_mean, response_mean))
colnames(plot_df) <- c("response","percent")

ggplot(data=plot_df) + geom_bar(mapping = aes(x=response, y=percent, fill=response), show.legend=FALSE, stat="identity")    

```
The graph shows that there is a definite component of positive response from customers that have not purchased insurance before.

The training dataset contains over 11 million rows while the test dataset contains over 7 million rows. We will be developing this analysis on a small laptop and
want to keep the the run time to be somewhat reasonable, so we will build subsets that will contain `r NUMBER_OF_TRAINING_ROWS` and `r NUMBER_OF_TEST_ROWS` records for the training and testing datasets respectively. There will be a total of `r NUMBER_OF_TEST_SETS` testing datasets sampled from the entire training set. When training and evaluating of the various models, the same datasets will be used.



```{r build subset, echo=FALSE, error=FALSE}
set.seed(2024)
train_set <- sample_n(train_data, NUMBER_OF_TRAINING_ROWS)
train_set_x <- train_set %>% select(-Response)
train_set_y <- as.factor(train_set$Response)
test_sets <- list()
for (i in 1:NUMBER_OF_TEST_SETS) {
  test_set  <- sample_n(train_data, NUMBER_OF_TEST_ROWS)
  test_sets <- list.append(test_sets, test_set)
}

```

We use the k-NN method of training which has a parameter of k. We examine the accuracy of the training for various values of k. We initially 
try a k in the set of `r seq(3, 11, 2)`

```{r first set of training, echo=FALSE}
k_hist <- list()
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn1 <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = c(seq(3,11,2),20)), trControl = control)

df_knn1 <- train_knn1$results     # extract results as a data frame
k_hist <- list.append(k_hist, df_knn1)

max_accuracy <- which.max(df_knn1$Accuracy)
max_k <- df_knn1$k[max_accuracy]

a <- max_k - 2
b <- max_k + 2
if (a < 1) a <- 1

# range <- seq(a,b)

```

```{r second set of training, echo = FALSE}

train_knn2 <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = seq(a,b)), trControl = control)

df_knn2 <- train_knn2$results
max_accuracy <- which.max(df_knn2$Accuracy)
max_k <- df_knn2$k[max_accuracy]

df_knn <- merge(df_knn1, df_knn2, by=c("Accuracy","k", "Kappa", "AccuracySD", "KappaSD"), all=TRUE)
```
```{r graghing value of k, echo=FALSE}
all_k <- full_join(df_knn1, df_knn2)
sorted_k <- all_k[order(all_k$k),]
ggplot(sorted_k, aes(x=k, y=Accuracy))+ geom_line()
max_k <- which.max(sorted_k$Accuracy)


```

From this second set of iterations, the maximum k value that we should use is `r max_k`. The final algorithm is calculated with this value.

```{r final train, echo=FALSE}
train_knn <- train(train_set_x, train_set_y, method="knn", tuneGrid = data.frame(k = max_k), trControl = control)

fit_knn <- knn3(train_set_x, train_set_y, k=max_k)

```
We picked a value of `r max_k` for our calculations. Let's examine the derivation of this number




Now consider the test set that we carved from the original data set. How does it match with the algorithm (function *predict*)? Here are the accuracy results for the k-NN model:

```{r test_sets, echo=FALSE, include=FALSE}

accum_accuracy <- 0
lapply(test_sets, knn_fcn)
print(paste("k-NN accuracy: ", as.character(accum_accuracy / NUMBER_OF_TEST_SETS)))

```

This algorithm ws based on the k-NN (nearest neighbor). Other models should be tested for comparisons. The first alternative to attempt is random forest, which is a compute-intensive model. There are several parameters that can be tuned to adjust the model and we will need to built a multitude of trees. As a result of this, we will only use a five-fold cross validation.

```{r random forest, echo = FALSE, include=FALSE}
accum_accuracy <- 0

test_set_x <- test_set %>% select(-Response)
test_set_y <- as.factor(test_set$Response)
control <- trainControl(method = "cv", number = 5)
grid <- data.frame(mtry = c(1,2,3))
train_rf <- train(train_set_x, train_set_y, method="rf", ntree = 500, trControl = control, tuneGrid = grid, nSamp = 10000)
fit_rf <- randomForest(train_set_x, train_set_y, mtry = train_rf$bestTune$mtry)
y_hat_rf <- predict(fit_rf, train_set_x, type="class")
cm <- confusionMatrix(y_hat_rf, factor(train_set_y))
cm$overall["Accuracy"]
cm$byClass["Sensitivity"]
cm$byClass["Specificity"]


lapply(test_sets, rf_fcn)

print(paste("random forest accuracy:", as.character(accum_accuracy / 10)))


```
The average of these 10 test cases is `r accum_accuracy`. We can see that the most important feature is whether the vehicle has been damaged followe closelyby whether the customer has been previous insured.
```{r graph importance}
importance <- fit_rf$importance
colnames(importance) <- c("Mean")
sorted_importance <- as.data.frame(importance[order(importance[,"Mean"]),]) * 100/ sum(importance)
colnames(sorted_importance) <- c("sorted_importance")


xlabls <- gsub("_", " ", rownames(sorted_importance))
ggplot(data=sorted_importance, aes(x = reorder(xlabls, -sorted_importance), y=sorted_importance, fill=reorder(xlabls, -sorted_importance))) + 
  geom_bar(show.legend=FALSE, stat="identity") +
  xlab("Feature") +
  ylab("Percent") +
  ylim(0, 50) +
  ggtitle("Relative Importance") +
  theme(plot.title = element_text(hjust = 0.5, size = 9)) +
  guides(x = guide_axis(angle = 90))  
```
    
### Ensemble
We can combine the two models that we have tried (knn and random forest) to determine if the combination is better than either one. 
```{r ensemble, echo=FALSE, include=FALSE}
ensemble_fcn <- function(test_set) {
  test_set_y <- as.factor(test_set$Response)
  test_set_x <- test_set %>% select(-Response)
  
  p_rf <- predict(fit_rf, test_set_x, type = "prob")
  p_rf <- p_rf / rowSums(p_rf)
  p_knn <- predict(fit_knn, test_set_x)
  p <- (p_rf + p_knn) / 2
  y_pred <- factor(apply(p, 1, which.max)-1)
  accum_accuracy <<- accum_accuracy + confusionMatrix(y_pred, test_set_y)$overall["Accuracy"]
  
}
accum_accuracy <- 0
lapply(test_sets, ensemble_fcn)
print(paste("Ensemble accuracy: ", as.character(accum_accuracy / 10)))


```

### Other models (taking defaults)
We've used two of the over 21,000 models that are available to determine from CRAN to determine a best fit for our data. Te first model that was investigated was glm or General Linear Model, which is a generalization of ordinary linear regression but accommodates distributions other than normal to include poisson, gaussian, binomial, multinominal, quasi, quasibinomial and gamma. Four of the families (binomial, gaussian, quasi and quasibinomial) produced the same accuracy result of 0.5928.


```{r glm test cases, echo=FALSE, include = FALSE}
glm_model(test_set, binomial())
glm_model(test_set, gaussian())
glm_model(test_set, quasi(link = "identity", variance ="constant"))
glm_model(test_set, quasibinomial(link = "logit"))


```

Another model that was tried was the naive Bayes model which seeks the model the distribution of inputs but does not learn which features are most important. Again, using the standard train set, we find that the accuracy from the confusion matrix is 0.7121, which is still lower that was obtained from either the k-NN model or the random forest model.

```{r naive_baues model testing}
n_bayes(train_set)

```
### Conclusion

That's all folks!
>>>>>>> b1014d9c10ce48d52dc522e94441a2285ab5d219
